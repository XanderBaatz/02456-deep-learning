{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ecfcd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tarfile\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Important for the import of cifar 10\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from dataloader import load_cifar10\n",
    "from dataloader import load_mnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3add80d",
   "metadata": {},
   "source": [
    "# Section 1: Dataloading + converting to variable with gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ef855",
   "metadata": {},
   "source": [
    "### Load the cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766fd6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (54000, 1, 28, 28) (54000,)\n",
      "Validation set: (6000, 1, 28, 28) (6000,)\n",
      "Test set: (10000, 1, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10\n",
    "#X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"../data/cifar-10-python.tar.gz\")\n",
    "# Load MNIST\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(\"/workspaces/02456-deep-learning/data/mnist\")\n",
    "\n",
    "### THE SHAPE OF CIFAR IS:    (RxGxB = 3) x 32 x 32       x      (no. pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d074134c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 784)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_flat = X_train.reshape(X_train.shape[0], -1)  # (54000, 784)\n",
    "X_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077deaa",
   "metadata": {},
   "source": [
    "So the input for each picture will be: 1 * 28 * 28 = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3a60f0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f00f054c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = X_train[0].flatten()\n",
    "ex.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00d611",
   "metadata": {},
   "source": [
    "# Section 2: FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87ad38",
   "metadata": {},
   "source": [
    "> ### Forward method of layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f341158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(activationName, Z):\n",
    "    if activationName == \"RELU\":\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    if activationName == \"Softmax\":\n",
    "        # x: (batch_size, n_classes)\n",
    "        x_shifted = Z - np.max(Z, axis=-1, keepdims=True)\n",
    "        e_x = np.exp(x_shifted)\n",
    "        return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2623516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_dim, output_dim, activation = \"RELU\"):\n",
    "        print(f'Layer with input_dim: {input_dim}, output dim, {output_dim}')\n",
    "        self.inp = input_dim\n",
    "        self.out = output_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        self.a = None\n",
    "        self.z = None\n",
    "        \n",
    "        #Vi ganger initialiseingne med meget små vægte, for at undgå at softmax bare bliver (0,1)\n",
    "        self.W = np.random.rand(output_dim, input_dim) * 0.01\n",
    "        self.b = np.random.rand(output_dim) * 0.01\n",
    "    \n",
    "    #This forward will either calculate for a single vector (z) or a batch\n",
    "    def forward(self, z):\n",
    "        # z can be (input_dim,) or (batch_size, input_dim)\n",
    "        if z.ndim == 1:\n",
    "            self.a = self.W @ z + self.b\n",
    "        else:\n",
    "            self.a = z @ self.W.T + self.b  # batch forward\n",
    "        self.z = activation(self.activation, self.a)\n",
    "        return self.z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d3667448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer with input_dim: 784, output dim, 5\n",
      "Layer with input_dim: 5, output dim, 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.21218628, 0.24674147, 0.13244197, 0.06904621, 0.01833682,\n",
       "       0.00784111, 0.07248769, 0.05375045, 0.01783634, 0.16933166])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = Layer(784, 5, \"RELU\")\n",
    "layer2 = Layer(5, 10, \"Softmax\")\n",
    "\n",
    "layer1.forward(X_flat)\n",
    "layer2.forward(layer1.z)\n",
    "\n",
    "layer2.z[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee90dae",
   "metadata": {},
   "source": [
    "> ### Forward method of layer + backwards "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e76ad0",
   "metadata": {},
   "source": [
    "1. We are working with multi-classification, so the only last layer activation function will be softmax\n",
    "2. We want to be able to use cross-entropy loss function\n",
    "\n",
    "$\\rightarrow$  We only have to derive  $\\frac{\\partial E_{cross}(z)}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cfce5",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "# --------------------\n",
    "# Activations\n",
    "# --------------------\n",
    "def sigmoid(Z):\n",
    "    return 1.0 / (1.0 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_prime(Z):\n",
    "    s = sigmoid(Z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_prime(Z):\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def tanh_prime(Z):\n",
    "    return 1 - np.tanh(Z)**2\n",
    "\n",
    "def softmax(Z):\n",
    "    Zc = Z - np.max(Z, axis=0, keepdims=True)\n",
    "    expZ = np.exp(Zc)\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "# map activation name to function/derivative\n",
    "_ACTIVATIONS = {\n",
    "    'sigmoid': (sigmoid, sigmoid_prime),\n",
    "    'relu':    (relu, relu_prime),\n",
    "    'tanh':    (tanh, tanh_prime),\n",
    "    'softmax': (softmax, None)  # softmax derivative handled together with CE loss\n",
    "}\n",
    "\n",
    "# --------------------\n",
    "# Initializer\n",
    "# --------------------\n",
    "class initializer:\n",
    "    @staticmethod\n",
    "    def init_weights(n_in: int, n_out: int, act_fn: str):\n",
    "        # He for ReLU, Xavier for tanh/sigmoid/softmax\n",
    "        if act_fn == 'relu':\n",
    "            return np.random.randn(n_out, n_in) * np.sqrt(2.0 / n_in)\n",
    "        else:\n",
    "            # Xavier/Glorot\n",
    "            return np.random.randn(n_out, n_in) * np.sqrt(1.0 / n_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_bias(n_out: int):\n",
    "        return np.zeros((n_out, 1))\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Utilities\n",
    "# --------------------\n",
    "def to_one_hot(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    y: shape (m,) with integer labels [0..num_classes-1]\n",
    "    returns: (num_classes, m)\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    Y = np.zeros((num_classes, m))\n",
    "    Y[y, np.arange(m)] = 1\n",
    "    return Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------\n",
    "# Layer (fully connected)\n",
    "# --------------------\n",
    "class Layer:\n",
    "    def __init__(self, n_in: int, n_out: int, act_fn: str = 'relu'):\n",
    "        assert act_fn in _ACTIVATIONS, f\"Unsupported activation '{act_fn}'\"\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.act_name = act_fn\n",
    "        self.act, self.act_prime = _ACTIVATIONS[act_fn]\n",
    "\n",
    "        # parameters\n",
    "        self.W = initializer.init_weights(n_in, n_out, act_fn)\n",
    "        self.b = initializer.init_bias(n_out)\n",
    "\n",
    "\n",
    "    def forward(self, A_prev: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        A_prev: (n_in, m)\n",
    "        returns A: (n_out, m)\n",
    "        \"\"\"\n",
    "        self.Z = np.dot(self.W, A_prev) + self.b  # (n_out, m)\n",
    "        if self.act_name == 'softmax':\n",
    "            self.A = self.act(self.Z)\n",
    "        else:\n",
    "            self.A = self.act(self.Z)\n",
    "        return self.A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared X_proc shape (features, m): (3072, 128)\n",
      "A_last shape (n_out, m): (10, 128)\n",
      "First 6 predicted labels: [2 6 6 2 2 2]\n",
      "First example softmax probs (first 10 classes):\n",
      "[2.23224749e-055 9.84803441e-090 1.00000000e+000 2.48666246e-128\n",
      " 2.23522770e-168 5.11786727e-195 9.03688840e-011 2.24619768e-100\n",
      " 3.98539195e-239 2.31292698e-197]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------\n",
    "# Neural Network\n",
    "# --------------------\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers: List[Layer] = []\n",
    "\n",
    "    def add_layer(self, n_in: int, n_out: int, act_fn: str = 'relu'):\n",
    "        \"\"\"\n",
    "        Add a fully-connected layer. Provide n_in for the layer.\n",
    "        (n_in must match previous layer's n_out if stacking.)\n",
    "        \"\"\"\n",
    "        layer = Layer(n_in, n_out, act_fn)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        X: (n_x, m)\n",
    "        returns A_last: (n_last, m)\n",
    "        \"\"\"\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        return A\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Example usage with flattened image input (e.g. CIFAR-10: 32*32*3 = 3072)\n",
    "# --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- load CIFAR-10 (you provided this) ---\n",
    "    # X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"../data/cifar-10-python.tar.gz\")\n",
    "    # For this example we assume the above line executed already and variables exist.\n",
    "\n",
    "    # Quick helper to convert many possible CIFAR shapes into (n_features, m)\n",
    "    def prepare_X_for_forward(X):\n",
    "        \"\"\"\n",
    "        Returns X_proc with shape (n_features, m)\n",
    "        Accepts:\n",
    "          - (3,32,32,m)\n",
    "          - (m,3,32,32)\n",
    "          - (m,32,32,3)\n",
    "          - (m, 3072)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim == 4:\n",
    "            # Case: (3,32,32,m)\n",
    "            if X.shape[0] == 3 and X.shape[1] == 32 and X.shape[2] == 32:\n",
    "                # reorder to (m, 3, 32, 32)\n",
    "                X = np.transpose(X, (3, 0, 1, 2))\n",
    "            # Now possible shapes: (m, 3, 32, 32) or (m, 32, 32, 3)\n",
    "            m = X.shape[0]\n",
    "            if X.shape[1] == 3 and X.shape[2] == 32 and X.shape[3] == 32:\n",
    "                # channels-first (m,3,32,32)\n",
    "                X_flat = X.reshape(m, -1)   # (m, 3072)\n",
    "            elif X.shape[1] == 32 and X.shape[2] == 32 and X.shape[3] == 3:\n",
    "                # channels-last (m,32,32,3)\n",
    "                X_flat = X.reshape(m, -1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unrecognized 4D shape: {X.shape}\")\n",
    "        elif X.ndim == 2:\n",
    "            # Already flattened e.g. (m, 3072) or (3072, m)\n",
    "            if X.shape[1] == 3072:\n",
    "                X_flat = X\n",
    "            elif X.shape[0] == 3072:\n",
    "                # already (3072, m) — transpose to (m,3072) for consistency below\n",
    "                X_flat = X.T\n",
    "            else:\n",
    "                raise ValueError(f\"Unrecognized 2D shape: {X.shape}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized X ndim: {X.ndim}\")\n",
    "\n",
    "        # Produce (n_features, m)\n",
    "        X_proc = X_flat.T  # now (3072, m)\n",
    "        return X_proc\n",
    "\n",
    "    # ---- prepare data ----\n",
    "    # Use a small batch for demo to save memory/time\n",
    "    # (you can swap in the full X_train if you want)\n",
    "    # Example: take first 128 train examples (if present)\n",
    "    num_examples = 128\n",
    "    try:\n",
    "        X_sample = X_train[:num_examples]\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"X_train not found. Make sure you ran load_cifar10() before this script.\")\n",
    "\n",
    "    X_proc = prepare_X_for_forward(X_sample)   # shape (3072, m_sample)\n",
    "    print(\"Prepared X_proc shape (features, m):\", X_proc.shape)\n",
    "\n",
    "    # ---- build a small FFN ----\n",
    "    nn = NeuralNetwork()\n",
    "    # Input layer must match flattened size 3072\n",
    "    nn.add_layer(3072, 512, act_fn='relu')\n",
    "    nn.add_layer(512, 256, act_fn='relu')\n",
    "    # Final layer for CIFAR-10 multiclass -> softmax with 10 outputs\n",
    "    nn.add_layer(256, 10, act_fn='softmax')\n",
    "\n",
    "    # ---- forward pass only ----\n",
    "    A_last = nn.forward(X_proc)   # shape (10, m_sample)\n",
    "    print(\"A_last shape (n_out, m):\", A_last.shape)\n",
    "\n",
    "    # If softmax final layer, show probabilities and predictions for first few examples\n",
    "    if nn.layers[-1].act_name == 'softmax':\n",
    "        probs = A_last  # already probabilities\n",
    "        preds = np.argmax(probs, axis=0)  # (m,)\n",
    "        print(\"First 6 predicted labels:\", preds[:6])\n",
    "        print(\"First example softmax probs (first 10 classes):\")\n",
    "        print(probs[:, 0])   # full vector of 10 probs for example 0\n",
    "    else:\n",
    "        # For binary output\n",
    "        print(\"First 6 outputs (sigmoid):\", A_last[:, :6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f2377",
   "metadata": {},
   "source": [
    "# Section 3: Loss / Backpropgation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a7e17",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
