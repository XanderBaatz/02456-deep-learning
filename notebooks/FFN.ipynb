{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1433,
   "id": "6ecfcd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tarfile\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Important for the import of cifar 10\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from dataloader import load_cifar10\n",
    "from dataloader import load_mnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678903a",
   "metadata": {},
   "source": [
    "#### **The point of this notebook, is to show the progress of building a FFN from sratch, that means each section will add a new functionality, so it will become a step-by-step progress. We have derived formulas and ideas in each section aswell**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3add80d",
   "metadata": {},
   "source": [
    "# Section 1: Dataloading + one-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ef855",
   "metadata": {},
   "source": [
    "### Load the cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1434,
   "id": "766fd6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (54000, 1, 28, 28) (54000,)\n",
      "Validation set: (6000, 1, 28, 28) (6000,)\n",
      "Test set: (10000, 1, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10\n",
    "#X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"../data/cifar-10-python.tar.gz\")\n",
    "# Load MNIST\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(\"/workspaces/02456-deep-learning/data/mnist\")\n",
    "\n",
    "### THE SHAPE OF CIFAR IS:    (RxGxB = 3) x 32 x 32       x      (no. pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1435,
   "id": "d074134c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 784)"
      ]
     },
     "execution_count": 1435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_flat = X_train.reshape(X_train.shape[0], -1)  # (54000, 784)\n",
    "X_flat = X_flat \n",
    "X_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077deaa",
   "metadata": {},
   "source": [
    "So the input for each picture will be: 1 * 28 * 28 = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1436,
   "id": "3a60f0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 1436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1437,
   "id": "f00f054c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 1437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = X_train[0].flatten()\n",
    "ex.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae1132",
   "metadata": {},
   "source": [
    "> Perform one hot encoding on the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1438,
   "id": "40cd30f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = y_train.max() + 1\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "\n",
    "print(y_train_onehot[0], y_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00d611",
   "metadata": {},
   "source": [
    "# Section 2: FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87ad38",
   "metadata": {},
   "source": [
    "> ### Forward method of layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1439,
   "id": "8f341158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(activationName, Z):\n",
    "    if activationName == \"RELU\":\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    if activationName == \"Softmax\":\n",
    "        # x: (batch_size, n_classes)\n",
    "        x_shifted = Z - np.max(Z, axis=-1, keepdims=True)\n",
    "        e_x = np.exp(x_shifted)\n",
    "        return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def activation_derivative(activationName, Z):\n",
    "    if activationName == \"RELU\":\n",
    "        return (Z > 0).astype(float)  # 1 where Z > 0, else 0\n",
    "    # add more derivatives if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1440,
   "id": "2623516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_dim, output_dim, activation = \"RELU\"):\n",
    "        print(f'Layer with input_dim: {input_dim}, output dim, {output_dim}')\n",
    "        self.inp = input_dim\n",
    "        self.out = output_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        self.a = None\n",
    "        self.z = None\n",
    "        \n",
    "        #HE init of weights (refer to lecture notes)\n",
    "        self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2 / input_dim)\n",
    "        self.b = np.zeros(output_dim)  \n",
    "\n",
    "    \n",
    "    #This forward will either calculate for a single vector (z) or a batch\n",
    "    def forward(self, z):\n",
    "        # z can be (input_dim,) or (batch_size, input_dim)\n",
    "        if z.ndim == 1:\n",
    "            self.a = self.W @ z + self.b\n",
    "        else:\n",
    "            self.a = z @ self.W.T + self.b  # batch forward\n",
    "        self.z = activation(self.activation, self.a)\n",
    "        return self.z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1441,
   "id": "d3667448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer with input_dim: 784, output dim, 15\n",
      "Layer with input_dim: 15, output dim, 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.57008414e-108, 2.97280935e-123, 3.06163905e-175, 9.99999996e-001,\n",
       "       1.26697188e-214, 6.31081518e-059, 1.04645255e-112, 4.85711669e-108,\n",
       "       3.84119188e-009, 3.81056810e-025])"
      ]
     },
     "execution_count": 1441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = Layer(784, 15, \"RELU\")\n",
    "output = Layer(15, 10, \"Softmax\")\n",
    "\n",
    "layer1.forward(X_flat)\n",
    "output.forward(layer1.z)\n",
    "\n",
    "output.z[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1442,
   "id": "876834de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.10118192e-068, 2.49912182e-134, 3.39637261e-087, ...,\n",
       "        8.43281655e-065, 2.75410843e-043, 3.94108824e-037],\n",
       "       [1.57008414e-108, 2.97280935e-123, 3.06163905e-175, ...,\n",
       "        4.85711669e-108, 3.84119188e-009, 3.81056810e-025],\n",
       "       [4.25462340e-006, 2.44917900e-069, 4.13370903e-183, ...,\n",
       "        7.02671888e-058, 6.39330370e-048, 2.04972702e-029],\n",
       "       ...,\n",
       "       [2.42166005e-088, 5.44552334e-245, 1.29506265e-220, ...,\n",
       "        2.14147236e-149, 4.02649274e-072, 1.00000000e+000],\n",
       "       [2.19173478e-057, 1.51458558e-057, 1.35091175e-107, ...,\n",
       "        2.12841428e-032, 2.24108121e-057, 7.80846641e-056],\n",
       "       [2.25363329e-023, 2.32524224e-032, 2.14889249e-046, ...,\n",
       "        9.99997356e-001, 4.26714922e-022, 2.63883038e-006]],\n",
       "      shape=(54000, 10))"
      ]
     },
     "execution_count": 1442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee90dae",
   "metadata": {},
   "source": [
    "> ### Forward method of layer + loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41efa1f",
   "metadata": {},
   "source": [
    "We are not getting a output, we are purely working with multi-class classfication, therefore we will use cross-entropy as a loss function. In order for us to calculate cross entropy loss\n",
    "\n",
    "1. We are getting a probability density as output\n",
    "2. onehot(score) gives a 10 x [0,0,0,0,0,1,0,0] of different combinations depending on label\n",
    "3. We then say loss(onehot(score), y_true) for each output\n",
    "\n",
    "> Source :  https://medium.com/@naqvishahwar120/ai-using-cross-entropy-loss-function-and-one-hot-encoding-4a102a1253dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2760af",
   "metadata": {},
   "source": [
    "We will be implementing cross entropy (numerical stable version from lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1443,
   "id": "08e7d8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1244/2274831616.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.sum(y_train_onehot * np.log(output.z)) / y_train_onehot.shape[0]\n",
      "/tmp/ipykernel_1244/2274831616.py:1: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -np.sum(y_train_onehot * np.log(output.z)) / y_train_onehot.shape[0]\n"
     ]
    }
   ],
   "source": [
    "loss = -np.sum(y_train_onehot * np.log(output.z)) / y_train_onehot.shape[0]\n",
    "\n",
    "print(\"Cross Entropy Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adceb3e4",
   "metadata": {},
   "source": [
    "> ### Forward method of layer + loss + Manual backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e76ad0",
   "metadata": {},
   "source": [
    "1. We are working with multi-classification, so the only last layer activation function will be softmax\n",
    "2. We want to be able to use cross-entropy loss function\n",
    "\n",
    ">Source: https://www.parasdahal.com/softmax-crossentropy\n",
    "\n",
    ">Source: https://www.kaggle.com/code/ravaghi/neural-networks-from-scratch-using-only-numpy\n",
    "\n",
    "> Overall method : Calcuate output gradient using Softmax / Cross entropy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e3757",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. Output layer\n",
    "\n",
    "The output logits:\n",
    "\n",
    "$$\n",
    "o_k = \\sum_i w^{(L)}_{ki}\\, z^{(L-1)}_i + b^{(L)}_k\n",
    "$$\n",
    "\n",
    "Softmax probabilities:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{o_i}}{\\sum_k e^{o_k}}\n",
    "$$\n",
    "\n",
    "Cross-entropy loss:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{y}, \\mathbf{p}) = - \\sum_k y_k \\log(p_k)\n",
    "$$\n",
    "\n",
    "Derivative of softmax + cross-entropy:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial o_i} = p_i - y_i\n",
    "$$\n",
    "\n",
    "Define the **output delta**:\n",
    "\n",
    "$$\n",
    "\\delta^{(L)}_i = p_i - y_i\n",
    "$$\n",
    "\n",
    "Weight and bias gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w^{(L)}_{ji}} = \\delta^{(L)}_j \\, z^{(L-1)}_i, \n",
    "\\qquad\n",
    "\\frac{\\partial L}{\\partial b^{(L)}_j} = \\delta^{(L)}_j\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Hidden layers (l = L-1, ..., 1)\n",
    "\n",
    "For hidden layer \\(l\\) with pre-activation \\(a^{(l)}_j\\) and post-activation \\(z^{(l)}_j = h_l(a^{(l)}_j)\\):\n",
    "\n",
    "Hidden delta:\n",
    "\n",
    "$$\n",
    "\\delta^{(l)}_j = h_l'(a^{(l)}_j) \\sum_{k=1}^{K_{l+1}} w^{(l+1)}_{kj} \\, \\delta^{(l+1)}_k\n",
    "$$\n",
    "\n",
    "Weight and bias gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w^{(l)}_{ji}} = \\delta^{(l)}_j \\, z^{(l-1)}_i, \n",
    "\\qquad\n",
    "\\frac{\\partial L}{\\partial b^{(l)}_j} = \\delta^{(l)}_j\n",
    "$$\n",
    "\n",
    "\n",
    "#### 3. Summary table\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Output layer:} \\quad & \n",
    "\\frac{\\partial L}{\\partial w^{(L)}_{ji}} = \\delta^{(L)}_j \\, z^{(L-1)}_i, \\quad\n",
    "\\frac{\\partial L}{\\partial b^{(L)}_j} = \\delta^{(L)}_j, \\quad\n",
    "\\delta^{(L)}_j = p_j - y_j \\\\\n",
    "\\text{Hidden layer:} \\quad & \n",
    "\\frac{\\partial L}{\\partial w^{(l)}_{ji}} = \\delta^{(l)}_j \\, z^{(l-1)}_i, \\quad\n",
    "\\frac{\\partial L}{\\partial b^{(l)}_j} = \\delta^{(l)}_j, \\quad\n",
    "\\delta^{(l)}_j = h_l'(a^{(l)}_j) \\sum_{k=1}^{K_{l+1}} w^{(l+1)}_{kj} \\, \\delta^{(l+1)}_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2555bab1",
   "metadata": {},
   "source": [
    "Manual backprop of 2 layer FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1444,
   "id": "38f644bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss before gradient descent: nan\n"
     ]
    }
   ],
   "source": [
    "output.z[0]\n",
    "print(\"Cross Entropy Loss before gradient descent:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1445,
   "id": "bfa78f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss after gradient descent: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1244/1192783709.py:29: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.sum(y_train_onehot * np.log(output.z)) / y_train_onehot.shape[0]\n",
      "/tmp/ipykernel_1244/1192783709.py:29: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -np.sum(y_train_onehot * np.log(output.z)) / y_train_onehot.shape[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for i in range(1):\n",
    "    m = X_flat.shape[0]\n",
    "\n",
    "    # Output layer\n",
    "    deltaL = output.z - y_train_onehot\n",
    "    dW2 = layer1.z.T @ deltaL / m\n",
    "    db2 = np.sum(deltaL, axis=0) / m\n",
    "    dW2[0], db2[0]\n",
    "\n",
    "    # Hidden layer (input layer)\n",
    "    deltaLm1 = deltaL @ output.W * activation_derivative(\"RELU\", layer1.a)\n",
    "    dW1 = X_flat.T @ deltaLm1 / m\n",
    "    db1 = np.sum(deltaLm1, axis=0) / m\n",
    "    dW1[0], db1[0]\n",
    "\n",
    "    layer1.W -= 0.01 * dW1.T\n",
    "    layer1.b -= 0.01 * db1\n",
    "\n",
    "    output.W -= 0.01 * dW2.T\n",
    "    output.b -= 0.01 * db2\n",
    "\n",
    "    layer1.forward(X_flat)\n",
    "    output.forward(layer1.z)\n",
    "    output.z[0]\n",
    "\n",
    "    loss = -np.sum(y_train_onehot * np.log(output.z)) / y_train_onehot.shape[0]\n",
    "        \n",
    "    #METRICS\n",
    "    loss_list.append(loss)\n",
    "\n",
    "    # ---------- Compute accuracy ----------\n",
    "    y_pred = np.argmax(output.z, axis=1)              \n",
    "    y_true = np.argmax(y_train_onehot, axis=1)\n",
    "    acc = np.mean(y_pred == y_true)\n",
    "    accuracy_list.append(acc)\n",
    "\n",
    "print(\"Cross Entropy Loss after gradient descent:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1446,
   "id": "08157fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: nan\n",
      "Final Accuracy: 0.10396296296296297\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Training Loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(accuracy_list)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Final Loss:\", loss_list[-1])\n",
    "print(\"Final Accuracy:\", accuracy_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b32f1",
   "metadata": {},
   "source": [
    "> **Part conclusion** We are able to see that the cross-entropy-loss is decreasing as we run the gradients and forward measures again, again and again, therefore we can create a loop for this. (called backprop). This will perform the gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa93921",
   "metadata": {},
   "source": [
    "> ### Forward method of layer + loss + modular backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a55f8e6",
   "metadata": {},
   "source": [
    "This will be a VERY simple NN architecture, in which we will build upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1447,
   "id": "c9653206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(activationName, Z):\n",
    "    if activationName == \"RELU\":\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    if activationName == \"Softmax\":\n",
    "        # x: (batch_size, n_classes)\n",
    "        x_shifted = Z - np.max(Z, axis=-1, keepdims=True)\n",
    "        e_x = np.exp(x_shifted)\n",
    "        return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def activation_derivative(activationName, Z):\n",
    "    if activationName == \"RELU\":\n",
    "        return (Z > 0).astype(float)  # 1 where Z > 0, else 0\n",
    "    # add more derivatives if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf923cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "\n",
    "    def returnLoss(self, y_onehot):\n",
    "        if self.loss == \"cross\":\n",
    "            return -np.sum(y_onehot * np.log(self.layers[-1].z)) / y_onehot.shape[0]\n",
    "\n",
    "    def linearLayer(self, input_dim, output_dim, activation = \"RELU\"):\n",
    "        self.layers.append(Layer(input_dim, output_dim, activation))\n",
    "\n",
    "    def Forward(self, input):\n",
    "        #First we calculate the input \n",
    "        self.layers[0].forward(input)\n",
    "\n",
    "        #Then we use the input z, to calculte subsquent layers\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].forward(self.layers[i-1].z)\n",
    "\n",
    "        #Output the output after softmax, this will be a distribution\n",
    "        print(self.layers[-1].z[0])\n",
    "\n",
    "    def backward(self,X_train, y_train_onehot):\n",
    "        #Used to find average in batching\n",
    "        m = X_train.shape[0]\n",
    "        last_layer = self.layers[-1]\n",
    "        \n",
    "        if loss == \"cross\":\n",
    "            delta = last_layer.z - y_train_onehot  # shape: (m, num_classes)\n",
    "\n",
    "                #Start from the end and go down\n",
    "                for i in range(len(reversed(self.layers))):\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1449,
   "id": "a42f8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_dim, output_dim, activation = \"RELU\"):\n",
    "        print(f'Layer with input_dim: {input_dim}, output dim, {output_dim}')\n",
    "        self.inp = input_dim\n",
    "        self.out = output_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        self.a = None\n",
    "        self.z = None\n",
    "        \n",
    "        #HE init of weights (refer to lecture notes)\n",
    "        self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2 / input_dim)\n",
    "        self.b = np.zeros(output_dim)  \n",
    "        \n",
    "        #These must be zero at intialization, but during back propagation we update them\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "    \n",
    "    #This forward will either calculate for a single vector (z) or a batch\n",
    "    def forward(self, z):\n",
    "        # z can be (input_dim,) or (batch_size, input_dim)\n",
    "        if z.ndim == 1:\n",
    "            self.a = self.W @ z + self.b\n",
    "        else:\n",
    "            self.a = z @ self.W.T + self.b  # batch forward\n",
    "        self.z = activation(self.activation, self.a)\n",
    "        return self.z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55554b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer with input_dim: 784, output dim, 15\n",
      "Layer with input_dim: 15, output dim, 15\n",
      "Layer with input_dim: 15, output dim, 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.36220029e-082 6.66243392e-009 1.79759878e-118 6.25174189e-086\n",
      " 1.05977921e-013 2.94465437e-050 1.68988697e-057 9.46048293e-062\n",
      " 1.28813071e-014 9.99999993e-001]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(118.44617744002967)"
      ]
     },
     "execution_count": 1450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipeline will be as following\n",
    "\n",
    "#Create the NN class, which takes care of forward, backward, losss etc\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "#Architecture\n",
    "nn.linearLayer(784, 15, \"RELU\")\n",
    "nn.linearLayer(15, 15, \"RELU\")\n",
    "nn.linearLayer(15, 10, \"Softmax\")\n",
    "\n",
    "#Forward measure\n",
    "nn.Forward(X_flat)  # (batch_size, 10)\n",
    "\n",
    "#Loss function\n",
    "nn.loss=\"cross\"\n",
    "nn.returnLoss(y_train_onehot)\n",
    "\n",
    "#Find gradients, using backwards\n",
    "nn.backward(X_flat, y_train_onehot)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665e33c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cfce5",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1451,
   "id": "a217f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "# --------------------\n",
    "# Activations\n",
    "# --------------------\n",
    "def sigmoid(Z):\n",
    "    return 1.0 / (1.0 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_prime(Z):\n",
    "    s = sigmoid(Z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_prime(Z):\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def tanh_prime(Z):\n",
    "    return 1 - np.tanh(Z)**2\n",
    "\n",
    "def softmax(Z):\n",
    "    Zc = Z - np.max(Z, axis=0, keepdims=True)\n",
    "    expZ = np.exp(Zc)\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "# map activation name to function/derivative\n",
    "_ACTIVATIONS = {\n",
    "    'sigmoid': (sigmoid, sigmoid_prime),\n",
    "    'relu':    (relu, relu_prime),\n",
    "    'tanh':    (tanh, tanh_prime),\n",
    "    'softmax': (softmax, None)  # softmax derivative handled together with CE loss\n",
    "}\n",
    "\n",
    "# --------------------\n",
    "# Initializer\n",
    "# --------------------\n",
    "class initializer:\n",
    "    @staticmethod\n",
    "    def init_weights(n_in: int, n_out: int, act_fn: str):\n",
    "        # He for ReLU, Xavier for tanh/sigmoid/softmax\n",
    "        if act_fn == 'relu':\n",
    "            return np.random.randn(n_out, n_in) * np.sqrt(2.0 / n_in)\n",
    "        else:\n",
    "            # Xavier/Glorot\n",
    "            return np.random.randn(n_out, n_in) * np.sqrt(1.0 / n_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_bias(n_out: int):\n",
    "        return np.zeros((n_out, 1))\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Utilities\n",
    "# --------------------\n",
    "def to_one_hot(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    y: shape (m,) with integer labels [0..num_classes-1]\n",
    "    returns: (num_classes, m)\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    Y = np.zeros((num_classes, m))\n",
    "    Y[y, np.arange(m)] = 1\n",
    "    return Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1452,
   "id": "76dbd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------\n",
    "# Layer (fully connected)\n",
    "# --------------------\n",
    "class Layer:\n",
    "    def __init__(self, n_in: int, n_out: int, act_fn: str = 'relu'):\n",
    "        assert act_fn in _ACTIVATIONS, f\"Unsupported activation '{act_fn}'\"\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.act_name = act_fn\n",
    "        self.act, self.act_prime = _ACTIVATIONS[act_fn]\n",
    "\n",
    "        # parameters\n",
    "        self.W = initializer.init_weights(n_in, n_out, act_fn)\n",
    "        self.b = initializer.init_bias(n_out)\n",
    "\n",
    "\n",
    "    def forward(self, A_prev: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        A_prev: (n_in, m)\n",
    "        returns A: (n_out, m)\n",
    "        \"\"\"\n",
    "        self.Z = np.dot(self.W, A_prev) + self.b  # (n_out, m)\n",
    "        if self.act_name == 'softmax':\n",
    "            self.A = self.act(self.Z)\n",
    "        else:\n",
    "            self.A = self.act(self.Z)\n",
    "        return self.A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "id": "fa18a22f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized 4D shape: (128, 1, 28, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1453], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train not found. Make sure you ran load_cifar10() before this script.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m X_proc \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_X_for_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# shape (3072, m_sample)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrepared X_proc shape (features, m):\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_proc\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# ---- build a small FFN ----\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1453], line 60\u001b[0m, in \u001b[0;36mprepare_X_for_forward\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     58\u001b[0m         X_flat \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized 4D shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Already flattened e.g. (m, 3072) or (3072, m)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3072\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized 4D shape: (128, 1, 28, 28)"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------\n",
    "# Neural Network\n",
    "# --------------------\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers: List[Layer] = []\n",
    "\n",
    "    def add_layer(self, n_in: int, n_out: int, act_fn: str = 'relu'):\n",
    "        \"\"\"\n",
    "        Add a fully-connected layer. Provide n_in for the layer.\n",
    "        (n_in must match previous layer's n_out if stacking.)\n",
    "        \"\"\"\n",
    "        layer = Layer(n_in, n_out, act_fn)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        X: (n_x, m)\n",
    "        returns A_last: (n_last, m)\n",
    "        \"\"\"\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        return A\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Example usage with flattened image input (e.g. CIFAR-10: 32*32*3 = 3072)\n",
    "# --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- load CIFAR-10 (you provided this) ---\n",
    "    # X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"../data/cifar-10-python.tar.gz\")\n",
    "    # For this example we assume the above line executed already and variables exist.\n",
    "\n",
    "    # Quick helper to convert many possible CIFAR shapes into (n_features, m)\n",
    "    def prepare_X_for_forward(X):\n",
    "        \"\"\"\n",
    "        Returns X_proc with shape (n_features, m)\n",
    "        Accepts:\n",
    "          - (3,32,32,m)\n",
    "          - (m,3,32,32)\n",
    "          - (m,32,32,3)\n",
    "          - (m, 3072)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim == 4:\n",
    "            # Case: (3,32,32,m)\n",
    "            if X.shape[0] == 3 and X.shape[1] == 32 and X.shape[2] == 32:\n",
    "                # reorder to (m, 3, 32, 32)\n",
    "                X = np.transpose(X, (3, 0, 1, 2))\n",
    "            # Now possible shapes: (m, 3, 32, 32) or (m, 32, 32, 3)\n",
    "            m = X.shape[0]\n",
    "            if X.shape[1] == 3 and X.shape[2] == 32 and X.shape[3] == 32:\n",
    "                # channels-first (m,3,32,32)\n",
    "                X_flat = X.reshape(m, -1)   # (m, 3072)\n",
    "            elif X.shape[1] == 32 and X.shape[2] == 32 and X.shape[3] == 3:\n",
    "                # channels-last (m,32,32,3)\n",
    "                X_flat = X.reshape(m, -1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unrecognized 4D shape: {X.shape}\")\n",
    "        elif X.ndim == 2:\n",
    "            # Already flattened e.g. (m, 3072) or (3072, m)\n",
    "            if X.shape[1] == 3072:\n",
    "                X_flat = X\n",
    "            elif X.shape[0] == 3072:\n",
    "                # already (3072, m)  transpose to (m,3072) for consistency below\n",
    "                X_flat = X.T\n",
    "            else:\n",
    "                raise ValueError(f\"Unrecognized 2D shape: {X.shape}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized X ndim: {X.ndim}\")\n",
    "\n",
    "        # Produce (n_features, m)\n",
    "        X_proc = X_flat.T  # now (3072, m)\n",
    "        return X_proc\n",
    "\n",
    "    # ---- prepare data ----\n",
    "    # Use a small batch for demo to save memory/time\n",
    "    # (you can swap in the full X_train if you want)\n",
    "    # Example: take first 128 train examples (if present)\n",
    "    num_examples = 128\n",
    "    try:\n",
    "        X_sample = X_train[:num_examples]\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"X_train not found. Make sure you ran load_cifar10() before this script.\")\n",
    "\n",
    "    X_proc = prepare_X_for_forward(X_sample)   # shape (3072, m_sample)\n",
    "    print(\"Prepared X_proc shape (features, m):\", X_proc.shape)\n",
    "\n",
    "    # ---- build a small FFN ----\n",
    "    nn = NeuralNetwork()\n",
    "    # Input layer must match flattened size 3072\n",
    "    nn.add_layer(3072, 512, act_fn='relu')\n",
    "    nn.add_layer(512, 256, act_fn='relu')\n",
    "    # Final layer for CIFAR-10 multiclass -> softmax with 10 outputs\n",
    "    nn.add_layer(256, 10, act_fn='softmax')\n",
    "\n",
    "    # ---- forward pass only ----\n",
    "    A_last = nn.forward(X_proc)   # shape (10, m_sample)\n",
    "    print(\"A_last shape (n_out, m):\", A_last.shape)\n",
    "\n",
    "    # If softmax final layer, show probabilities and predictions for first few examples\n",
    "    if nn.layers[-1].act_name == 'softmax':\n",
    "        probs = A_last  # already probabilities\n",
    "        preds = np.argmax(probs, axis=0)  # (m,)\n",
    "        print(\"First 6 predicted labels:\", preds[:6])\n",
    "        print(\"First example softmax probs (first 10 classes):\")\n",
    "        print(probs[:, 0])   # full vector of 10 probs for example 0\n",
    "    else:\n",
    "        # For binary output\n",
    "        print(\"First 6 outputs (sigmoid):\", A_last[:, :6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f2377",
   "metadata": {},
   "source": [
    "# Section 3: Loss / Backpropgation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a7e17",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
